{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94c25dc8-18db-4931-8d8f-8a833af4c6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.utils import to_categorical\n",
    "from torch.utils.data import TensorDataset\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error,confusion_matrix,f1_score\n",
    "from utils.function import *\n",
    "from models.de_PAMAP2 import DAE\n",
    "from models.activity_recognition import *\n",
    "# from models.activity_recognition import get_eval_model\n",
    "from models.dis_z import DIS_Z\n",
    "import copy\n",
    "#from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27d944f1-784f-4210-9546-deaf048b6fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.root = '../data'\n",
    "        self.batchSize = 64\n",
    "        self.maxEpochs = 100\n",
    "        self.nz = 200\n",
    "        self.lr = 1e-4\n",
    "        self.fSize = 64\n",
    "        self.outDir = 'data/experiments/DE_PAMAP'\n",
    "        self.commit = 'eval'\n",
    "        self.alpha = 1.0\n",
    "        # self.sigma = 0.35\n",
    "        self.M = 5\n",
    "        self.loss = 'MSE' #'BCE'\n",
    "        self.loadDAE = False\n",
    "        self.loadSVM = False    \n",
    "        self.load_DAE_from = None\n",
    "        self.evalMode = False\n",
    "        self.comment = ''\n",
    "        self.momentum = 0.1\n",
    "        self.c = 0.01\n",
    "        self.svmLR = 1e-4\n",
    "        self.Ntest = 100\n",
    "        self.gpuNo = 3\n",
    "        self.multimodalZ = False\n",
    "        self.window_len = 512\n",
    "        self.stride_len = 20\n",
    "        self.act_list = [1, 2, 3, 4, 5, 6, 7, 12, 13, 16, 17, 24]\n",
    "        self.imSize = 64\n",
    "        # self.sigma = [50, 80]\n",
    "        # self.sigma = [0.2, 50, 80]\n",
    "        self.sigma=0.05\n",
    "        # self.cuda_id = 2\n",
    "        self.random_seed = 2\n",
    "        self.train_split = 0.8\n",
    "\n",
    "        #self.corr= 'ZeroMask' # options: Gaussian, ZeroMask, ConsecutiveZeros\n",
    "\n",
    "        # path for corruption \"consecutive interval\"sigma [60,80]\n",
    "        self.dae_model_loc = \"PAMAP2_Dataset/Optional/Ex_2\" \n",
    "\n",
    "\n",
    "        # self.ar_model_loc = \"data/experiments/DAAE1000/Ex_136/ar_params\"\n",
    "        self.ar_model_loc = \"pamap2_ConvAttn_ep0.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4e6f4ee-8711-4258-a933-55e1bf8e6d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "gpu_id = 3\n",
    "\n",
    "if gpu_id>=0:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "    cuda_id = \"cuda:\" + str(0)  # cuda:2\n",
    "\n",
    "device = torch.device(cuda_id if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "if (torch.cuda.is_available()):\n",
    "    torch.cuda.set_device(cuda_id)\n",
    "    print(\"Current GPU ID:\", torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff755101-ccb9-44fb-9b83-7011f4c7ce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dis(args, dae, multimodalZ):\n",
    "    if not multimodalZ:\n",
    "        print('\\n ** USING NORMAL PRIOR **')\n",
    "        prior = dae.norm_prior\n",
    "        NZ = args.nz\n",
    "    else:\n",
    "        print('\\n ** USING MULTIMODAL PRIOR **')\n",
    "        prior = dae.multi_prior\n",
    "        NZ = 2\n",
    "    dis = DIS_Z(nz=NZ, prior=prior)\n",
    "\n",
    "    return dis, NZ\n",
    "\n",
    "\n",
    "def analysis(args, dae, testDataset, X, num_classes):\n",
    "    # Prepare testdata set, drop the last incomplete batch\n",
    "    test_x = torch.zeros(len(testDataset), X.shape[1], X.shape[2], X.shape[3])\n",
    "    test_labels = torch.zeros(len(testDataset), num_classes)\n",
    "    for test_id in range(len(testDataset)):\n",
    "        test_labels[test_id] = testDataset[test_id][1]\n",
    "        test_x[test_id] = testDataset[test_id][0]\n",
    "    print(test_x.shape)\n",
    "\n",
    "    # Corrupt dataset\n",
    "    corr_test_x = dae.corrupt(test_x)\n",
    "    \n",
    "    mean_fill_test = interpolation_meanfilling(corr_test_x)\n",
    "    linear_interp_test = linear_interpolation(corr_test_x)\n",
    "# recon_test -> synthesize the entire dataset\n",
    "    z = dae.encode(corr_test_x)\n",
    "    recon_test = dae.decode(z)\n",
    "\n",
    "    # breakpoint()\n",
    "# reconstruct filling testset (fill missing values only, not valid for noisy data)\n",
    "    recon_fill_test = copy.deepcopy(corr_test_x).detach().cpu().numpy()\n",
    "    np.copyto(recon_fill_test, recon_test.detach().numpy(), where = recon_fill_test==0)\n",
    "    recon_fill_test = torch.from_numpy(recon_fill_test)\n",
    "\n",
    "    raw_test_dataset = TensorDataset(test_x, test_labels)\n",
    "    corr_test_dataset = TensorDataset(corr_test_x, test_labels)\n",
    "    recon_test_dataset = TensorDataset(recon_test, test_labels)\n",
    "    recon_fill_test_dataset = TensorDataset(recon_fill_test, test_labels)\n",
    "    mean_fill_test_dataset = TensorDataset(mean_fill_test, test_labels)\n",
    "    linear_interp_test_dataset = TensorDataset(linear_interp_test, test_labels)\n",
    "\n",
    "    return test_x, corr_test_x, recon_test, recon_fill_test, mean_fill_test, linear_interp_test, raw_test_dataset, corr_test_dataset, recon_test_dataset, recon_fill_test_dataset, mean_fill_test_dataset,linear_interp_test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac638067-7240-4159-b37b-c2532899ca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolation_meanfilling(corr_test_x):\n",
    "    mean_fill_test = copy.deepcopy(corr_test_x).detach().cpu().numpy()\n",
    "    for i in range(mean_fill_test.shape[0]):\n",
    "        for j in range(mean_fill_test.shape[2]):\n",
    "            if np.count_nonzero(mean_fill_test[i,:,j,:]) == 0:  \n",
    "                ch_mean = 0\n",
    "            else:\n",
    "             #ch_mean = np.sum(mean_fill_test[i][0][j]) / np.count_nonzero(mean_fill_test[i][0][j])\n",
    "                ch_mean = np.sum(mean_fill_test[i,:,j,:]) / np.count_nonzero(mean_fill_test[i,:,j,:])\n",
    "                mean_fill_test[i,:,j,:][mean_fill_test[i,:,j,:] == 0] = ch_mean\n",
    "    mean_fill_test = torch.from_numpy(mean_fill_test)\n",
    "    return mean_fill_test\n",
    "\n",
    "\n",
    "def linear_interpolation(corr_test_x):\n",
    "    linear_interp_test = copy.deepcopy(corr_test_x).detach().cpu().numpy()\n",
    "    # corr_text_x shape is 18944, 1, 27, 171\n",
    "    # expected shape 18944, 171, 27\n",
    "    # linear_interp_test = linear_interp_test.reshape(-1, 171,27)\n",
    "    linear_interp_test = linear_interp_test.reshape(-1, linear_interp_test.shape[3],linear_interp_test.shape[2])\n",
    "    for i in range(linear_interp_test.shape[0]):\n",
    "        for j in range(linear_interp_test.shape[2]):\n",
    "            if np.count_nonzero(linear_interp_test[i,:,j]) == 0: # when all data points in this channel are missing\n",
    "                linear_interp_test[i, :, j] = 0.0\n",
    "            else:\n",
    "                idxs = np.arange(linear_interp_test.shape[1]) # indexes of all the samples\n",
    "                zero_filter = linear_interp_test[i,:,j] == 0 # index filter for zero values\n",
    "                zero_idxs = idxs[zero_filter] # indexes for zero values\n",
    "                non_zero_idxs = idxs[~zero_filter] # xp, indexes for non-zero values\n",
    "                non_zero_vals = linear_interp_test[i, ~zero_filter,j] # fp, non-zero values\n",
    "                interp_vals = np.interp(zero_idxs, non_zero_idxs, non_zero_vals) # interpolated values\n",
    "                linear_interp_test[i,zero_idxs,j] = interp_vals # fill interpolated values to the corrupted signal\n",
    "    linear_interp_test = torch.from_numpy(linear_interp_test)\n",
    "    linear_interp_test = torch.reshape(linear_interp_test, (-1, 1, linear_interp_test.shape[2], linear_interp_test.shape[1]))\n",
    "    return linear_interp_test\n",
    "\n",
    "def evaluate_rmse(corr_test_x, recon_test, recon_fill_test,mean_fill_test,linear_interp_test,test_x):\n",
    "    corr_rms = mean_squared_error(test_x.reshape(test_x.shape[0],-1).cpu().detach().numpy(), corr_test_x.reshape(corr_test_x.shape[0],-1).cpu().detach().numpy(), squared=False)\n",
    "    print('Corr RMSE:\\n' + str(corr_rms))\n",
    "\n",
    "    recon_rms = mean_squared_error(test_x.reshape(test_x.shape[0],-1).cpu().detach().numpy(), recon_test.reshape(recon_test.shape[0],-1).cpu().detach().numpy(), squared=False)\n",
    "    print('Recon RMSE:\\n' + str(recon_rms))\n",
    "\n",
    "    recon_fill_rms = mean_squared_error(test_x.reshape(test_x.shape[0],-1).cpu().detach().numpy(), recon_fill_test.reshape(recon_fill_test.shape[0],-1).cpu().detach().numpy(), squared=False)\n",
    "    print('Recon fill RMSE:\\n' + str(recon_fill_rms))\n",
    "\n",
    "    mean_fill_rms = mean_squared_error(test_x.reshape(test_x.shape[0],-1).cpu().detach().numpy(), mean_fill_test.reshape(mean_fill_test.shape[0],-1).cpu().detach().numpy(), squared=False)\n",
    "    print('Mean Fill RMSE:\\n' + str(mean_fill_rms))\n",
    "    \n",
    "    linear_interp_rms = mean_squared_error(test_x.reshape(test_x.shape[0],-1), linear_interp_test.reshape(linear_interp_test.shape[0],-1).cpu().detach().numpy(), squared=False)\n",
    "    print('Linear Interpolation RMSE:\\n' + str(linear_interp_rms))\n",
    "    return \n",
    "#def plot(test_x, corr_test_x, recon_test)\n",
    "\n",
    "def plot(test_x, corr_test_x, recon_test, recon_fill_test,mean_fill_test, linear_interp_test):\n",
    "    plt.imshow(test_x[0][0].detach())\n",
    "    plt.title('Raw Data')\n",
    "   # plt.subplot(4,1,1)\n",
    "    plt.savefig(join(args.dae_model_loc, 'raw.png'))\n",
    "\n",
    "    plt.imshow(corr_test_x[0][0].detach())\n",
    "    plt.title('Corrupted')\n",
    "   # plt.subplot(4,1,2)\n",
    "    plt.savefig(join(args.dae_model_loc, 'corr.png'))\n",
    "\n",
    "    plt.imshow(recon_test[0][0].detach())\n",
    "    plt.title('Reconstructed')\n",
    "   # plt.subplot(4,1,3)\n",
    "    plt.savefig(join(args.dae_model_loc, 'reconstructed.png'))\n",
    "\n",
    "    plt.imshow(recon_fill_test[0][0].detach())\n",
    "    plt.title('Reconstructed fill')\n",
    "   # plt.subplot(4,1,3)\n",
    "    plt.savefig(join(args.dae_model_loc, 'rec_fill.png'))\n",
    "\n",
    "    plt.imshow(mean_fill_test[0][0].detach())\n",
    "    plt.title('Mean Fill')\n",
    "   # plt.subplot(4,1,4)\n",
    "    plt.savefig(join(args.dae_model_loc, 'mean.png'))\n",
    "    \n",
    "    plt.imshow(linear_interp_test[0][0].detach())\n",
    "    plt.title('Linear Interp')\n",
    "   # plt.subplot(4,1,4)\n",
    "    plt.savefig(join(args.dae_model_loc, 'linear.png'))\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def test_dae(args, path, trainLoader, testLoader):\n",
    "    pass\n",
    "\n",
    "\n",
    "def test_activity_recognition(args, path, trainLoader, testLoader):\n",
    "    pass\n",
    "\n",
    "\n",
    "def calculate_combined_accuracy(args, test_loader, sigma):\n",
    "    # device = torch.device(args.gpuNo if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    #ar = ActivityRecognitionCNN(len(args.act_list))\n",
    "    # ar = get_eval_model(len(args.act_list), model_path=args.ar_model_loc)\n",
    "    #ar.load_state_dict(torch.load(args.ar_model_loc))\n",
    "    args.n_sensor_channels = 27\n",
    "    args.len_seq = 171\n",
    "    args.num_classes=12\n",
    "    \n",
    "    ar = get_eval_model(n_sensor_channels=args.n_sensor_channels, len_seq=args.len_seq, num_classes=args.num_classes, model_path=args.ar_model_loc) #n_sensor_channels, len_seq, num_classes, model_path\n",
    "    ar = ar.to(device)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_true = []\n",
    "    total_pred = []\n",
    "\n",
    "    with torch.no_grad():   \n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)  \n",
    "            # print(images.shape)\n",
    "            \n",
    "            outputs = ar(images)\n",
    "\n",
    "            # _, predicted = torch.max(outputs.data, 1)\n",
    "            predicted = torch.argmax(outputs.data, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == torch.argmax(labels, dim=1)).sum().item()\n",
    "            \n",
    "            total_pred = total_pred + predicted.cpu().numpy().tolist()\n",
    "            total_true = total_true + (torch.argmax(labels, dim=1).cpu().numpy().tolist())\n",
    "            \n",
    "    print(f'Test Accuracy:\\n{100.0 * correct / total}')\n",
    "    \n",
    "    # print(\" | \".join(act_labels_txt))\n",
    "    conf_mat = confusion_matrix(y_true = total_true, y_pred = total_pred)\n",
    "    conf_mat = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]\n",
    "    print(np.array(conf_mat).round(3) * 100)  \n",
    "    f1 = f1_score(y_true = total_true, y_pred = total_pred, average='weighted')\n",
    "    print('F1 score:\\n', f1)\n",
    "    print('')\n",
    "    return \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7439c1-2981-41bd-88b1-1af069b44300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe668ed-2364-4e3f-86c2-da1dc345808d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd61961-f10b-4720-afe9-118da160d3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f846e27-c581-4188-a91f-50a5313dc137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Entered\n",
      "loading params...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'PAMAP2_Dataset/Optional/Ex_2\\\\dae_params'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 25\u001b[0m\n\u001b[0;32m     21\u001b[0m testDataset \u001b[39m=\u001b[39m TensorDataset(X, labels)\n\u001b[0;32m     24\u001b[0m dae \u001b[39m=\u001b[39m DAE(nz\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mnz, imSize\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mimSize, fSize\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mfSize, multimodalZ\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mmultimodalZ)\n\u001b[1;32m---> 25\u001b[0m dae\u001b[39m.\u001b[39;49mload_params(args\u001b[39m.\u001b[39;49mdae_model_loc)\n\u001b[0;32m     27\u001b[0m \u001b[39m# test_dae(args, dae_path, trainLoader, testLoader)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[39m# test_activity_recognition(args, ar_path, trainLoader, testLoader)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \n\u001b[0;32m     30\u001b[0m \u001b[39m#acc = calculate_combined_accuracy(args, testLoader, sigma=0.5)\u001b[39;00m\n\u001b[0;32m     31\u001b[0m test_x, corr_test_x, recon_test, recon_fill_test, mean_fill_test, linear_interp_test, raw_test_dataset, corr_test_dataset, recon_test_dataset, recon_fill_test_dataset, mean_fill_test_dataset, linear_interp_test_dataset \u001b[39m=\u001b[39m analysis(args, dae, testDataset, X_test, args\u001b[39m.\u001b[39mnum_classes)\n",
      "File \u001b[1;32mc:\\Users\\Arka Rutvik\\OneDrive\\Desktop\\New folder (2)\\DAE\\models\\de_PAMAP2.py:210\u001b[0m, in \u001b[0;36mDAE.load_params\u001b[1;34m(self, exDir)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_params\u001b[39m(\u001b[39mself\u001b[39m, exDir):\n\u001b[0;32m    209\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mloading params...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 210\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(join(exDir, \u001b[39m'\u001b[39;49m\u001b[39mdae_params\u001b[39;49m\u001b[39m'\u001b[39;49m),map_location\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcuda:0\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\Users\\Arka Rutvik\\OneDrive\\Desktop\\New folder (2)\\Trial\\Lib\\site-packages\\torch\\serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    789\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 791\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m    792\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    793\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    794\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    796\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\Arka Rutvik\\OneDrive\\Desktop\\New folder (2)\\Trial\\Lib\\site-packages\\torch\\serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 271\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    272\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    273\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\Arka Rutvik\\OneDrive\\Desktop\\New folder (2)\\Trial\\Lib\\site-packages\\torch\\serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[1;32m--> 252\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'PAMAP2_Dataset/Optional/Ex_2\\\\dae_params'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args = Args()\n",
    "    if args.multimodalZ:\n",
    "        args.nz = 2\n",
    "        \n",
    "    args.num_classes=12\n",
    "    random.seed(args.random_seed)\n",
    "    np.random.seed(args.random_seed)\n",
    "    torch.manual_seed(args.random_seed)\n",
    "\n",
    "    # X, labels = prepare_data(args)\n",
    "    # trainDataset, testDataset, trainLoader, testLoader = prepare_dataloaders(args, X, labels)\n",
    "    X_train, X_test, y_train, y_test = prepare_data_PAMAP2(args)\n",
    "    \n",
    "    tail = len(X_test) % args.batchSize\n",
    "    X = torch.from_numpy(X_test[:len(X_test)-tail,:,:])\n",
    "    labels = torch.from_numpy(y_test[:len(X_test)-tail,:])\n",
    "    # X, labels = prepare_data(args)\n",
    "    # trainDataset, testDataset, trainLoader, testLoader = prepare_dataloaders(args, X, labels)\n",
    "    \n",
    "    testDataset = TensorDataset(X, labels)\n",
    "    \n",
    "    \n",
    "    dae = DAE(nz=args.nz, imSize=args.imSize, fSize=args.fSize, multimodalZ=args.multimodalZ)\n",
    "    dae.load_params(args.dae_model_loc)\n",
    "\n",
    "    # test_dae(args, dae_path, trainLoader, testLoader)\n",
    "    # test_activity_recognition(args, ar_path, trainLoader, testLoader)\n",
    "\n",
    "    #acc = calculate_combined_accuracy(args, testLoader, sigma=0.5)\n",
    "    test_x, corr_test_x, recon_test, recon_fill_test, mean_fill_test, linear_interp_test, raw_test_dataset, corr_test_dataset, recon_test_dataset, recon_fill_test_dataset, mean_fill_test_dataset, linear_interp_test_dataset = analysis(args, dae, testDataset, X_test, args.num_classes)\n",
    "    # test_x, corr_test_x, recon_test, recon_fill_test, mean_fill_test, raw_test_dataset, corr_test_dataset, recon_test_dataset, recon_fill_test_dataset, mean_fill_test_dataset = analysis(dae, testDataset, labels)\n",
    "    #corr_test_x, test_x, recon_test, raw_test_dataset, corr_test_dataset, recon_test_dataset = analysis(dae, testDataset, labels)\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"Raw testset:\")\n",
    "    raw_test_loader = torch.utils.data.DataLoader(raw_test_dataset,\n",
    "    batch_size= args.batchSize, shuffle=False)\n",
    "    calculate_combined_accuracy(args, raw_test_loader, sigma=args.sigma)\n",
    "\n",
    "    print(\"Corrupted testset:\")\n",
    "    corr_test_loader = torch.utils.data.DataLoader(corr_test_dataset,\n",
    "    batch_size= args.batchSize, shuffle=False)\n",
    "    calculate_combined_accuracy(args,corr_test_loader, sigma=args.sigma)\n",
    "\n",
    "    print(\"Reconstructed testset:\")\n",
    "    recon_test_loader = torch.utils.data.DataLoader(recon_test_dataset,\n",
    "    batch_size=args.batchSize, shuffle=False)\n",
    "    calculate_combined_accuracy(args,recon_test_loader, sigma=args.sigma)\n",
    "\n",
    "\n",
    "    print(\"Reconstructed fill testset:\")\n",
    "    recon_fill_test_loader = torch.utils.data.DataLoader(recon_fill_test_dataset,\n",
    "    batch_size=args.batchSize, shuffle=False)\n",
    "    calculate_combined_accuracy(args,recon_fill_test_loader, sigma=args.sigma)\n",
    "\n",
    "\n",
    "    print(\"Mean Fill testset\")\n",
    "    mean_fill_test_loader = torch.utils.data.DataLoader(mean_fill_test_dataset,\n",
    "    batch_size=args.batchSize, shuffle=False)\n",
    "    calculate_combined_accuracy(args,mean_fill_test_loader, sigma=args.sigma)\n",
    "    \n",
    "    print(\"Linear Interpolation testset:\")\n",
    "    linear_interp_test_loader = torch.utils.data.DataLoader(linear_interp_test_dataset,\n",
    "    batch_size=args.batchSize, shuffle=False)\n",
    "    calculate_combined_accuracy(args,linear_interp_test_loader, sigma=args.sigma)\n",
    "\n",
    "    evaluate_rmse(corr_test_x, recon_test,recon_fill_test, mean_fill_test, linear_interp_test,test_x)\n",
    "    plot(test_x, corr_test_x, recon_test,recon_fill_test, mean_fill_test, linear_interp_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38740a4d-b9e5-4f15-8fda-8e1f56a26a74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e4b8b5-9d28-4f96-8037-1ef363b665ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
