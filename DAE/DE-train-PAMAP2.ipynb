{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d7460ff-3b86-4bfd-8ca7-2f476ef7b595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from os.path import join\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.utils import to_categorical\n",
    "from torch import optim\n",
    "from torch.nn import BCELoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from models.de_PAMAP2 import DAE\n",
    "from models.dis_z import DIS_Z\n",
    "from models.activity_recognition import *\n",
    "from utils.function import *\n",
    "\n",
    "# from matplotlib import pyplot as plt\n",
    "# %pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cf7560-d29e-45f4-a9a4-ea820197626e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668870a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1f74b96-c0f1-49f7-80e3-fec97098e536",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.root = '../data'\n",
    "        self.batchSize = 64\n",
    "        self.maxEpochs = 100\n",
    "        self.nz = 200\n",
    "        self.lr = 1e-4\n",
    "        self.fSize = 64\n",
    "        self.outDir = '../PAMAP2_DATASET/Optional/'\n",
    "        self.commit = 'eval'\n",
    "        self.alpha = 1.0\n",
    "        # self.sigma = 0.35\n",
    "        self.M = 5\n",
    "        self.loss = 'MSE' #'BCE'\n",
    "        self.loadDAE = False\n",
    "        self.loadSVM = False    \n",
    "        self.load_DAE_from = None\n",
    "        self.evalMode = False\n",
    "        self.comment = ''\n",
    "        self.momentum = 0.1\n",
    "        self.c = 0.01\n",
    "        self.svmLR = 1e-4\n",
    "        self.Ntest = 100\n",
    "        self.gpuNo = 0\n",
    "        self.multimodalZ = False\n",
    "        self.window_len = 512\n",
    "        self.stride_len = 20\n",
    "        self.act_list = [1, 2, 3, 4, 5, 6, 7, 12, 13, 16, 17, 24]\n",
    "        self.imSize = 64\n",
    "        self.sigma = [40, 80]\n",
    "        # self.sigma = 0.1\n",
    "        # self.sigma = [0.1, 40, 80]\n",
    "        self.cuda_id = 0\n",
    "        self.random_seed = 2\n",
    "        self.train_split = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "55728fc3-e00f-4aff-9d7d-258842ef04a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dis(args, dae, multimodalZ):\n",
    "    if not multimodalZ:\n",
    "        print('\\n ** USING NORMAL PRIOR **')\n",
    "        prior = dae.norm_prior\n",
    "        NZ = args.nz\n",
    "    else:\n",
    "        print('\\n ** USING MULTIMODAL PRIOR **')\n",
    "        prior = dae.multi_prior\n",
    "        NZ = 2\n",
    "    dis = DIS_Z(nz=NZ, prior=prior)\n",
    "\n",
    "    return dis, NZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "38fab823-f472-4620-bf9e-29e8ba54a792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dae(args, exDir, trainLoader, testLoader):\n",
    "    dae = DAE(nz=args.nz, imSize=args.imSize, fSize=args.fSize, sigma=args.sigma, multimodalZ=args.multimodalZ) #sigma=level of corruption\n",
    "    # dis, NZ = build_dis(args, dae=dae, multimodalZ=args.multimodalZ)\n",
    "    \n",
    "    if dae.useCUDA:\n",
    "        torch.cuda.set_device(args.gpuNo)\n",
    "        dae.cuda()\n",
    "        # dis.cuda()\n",
    "    \n",
    "    save_input_args(exDir, args)  #save training opts\n",
    "    save_exp_details(args, exDir)\n",
    "    \n",
    "    # #Create optimizers\n",
    "    optimDAE = optim.RMSprop(dae.parameters(), lr = args.lr, momentum=args.momentum)\n",
    "    # optimDIS = optim.RMSprop(dis.parameters(), lr = args.lr, momentum=args.momentum)\n",
    "\n",
    "    # #Keeping track of training\n",
    "    losses = {'enc': [], 'rec': [], 'dis':[], 'test rec':[]}\n",
    "\n",
    "    # xTest, yTest = prep_data(iter(testLoader).next(), useCUDA=dae.useCUDA)\n",
    "\n",
    "    with torch.autograd.set_detect_anomaly(True):\n",
    "        for e in range(args.maxEpochs):\n",
    "\n",
    "            epochEncLoss=0\n",
    "            epochRecLoss=0\n",
    "            epochDisLoss=0\n",
    "\n",
    "            for i, data in enumerate(trainLoader):\n",
    "                # print(i)\n",
    "\n",
    "                T = time()\n",
    "\n",
    "                dae.train()\n",
    "                # dis.train()\n",
    "\n",
    "                x, y = prep_data(data, useCUDA=dae.useCUDA)\n",
    "                # print(x.shape)\n",
    "            \n",
    "                # get outputs\n",
    "                zFake, xRec = dae.forward(x)\n",
    "\n",
    "                # clac losses\n",
    "                recLoss = dae.rec_loss(xRec, x, loss=args.loss)  #loss='BCE' or 'MSE'\n",
    "            \n",
    "                optimDAE.zero_grad()\n",
    "                recLoss.backward()\n",
    "                optimDAE.step()            \n",
    "            \n",
    "                # zFake, xRec = dae.forward(x)    \n",
    "            \n",
    "                # disLoss = dis.dis_loss(zFake)\n",
    "                #do updates\n",
    "\n",
    "                # optimDIS.zero_grad()\n",
    "                # disLoss.backward()\n",
    "                # optimDIS.step()\n",
    "            \n",
    "#                 encLoss = dis.gen_loss(zFake)\n",
    "            \n",
    "#                 optimDAE.zero_grad()\n",
    "#                 encLoss.backward()\n",
    "#                 optimDAE.step()                   \n",
    "\n",
    "\n",
    "                # epochEncLoss+=encLoss.detach().cpu().numpy()\n",
    "                epochRecLoss+=recLoss.detach().cpu().numpy()\n",
    "                # epochDisLoss+=disLoss.detach().cpu().numpy()\n",
    "            \n",
    "                if i%100 == 0:\n",
    "                    print('[%d, %d] rec: %0.5f, time: %0.3f' % (e, i, recLoss.detach().cpu().numpy(), time() - T))\n",
    "                    # print('[%d, %d] enc: %0.5f, rec: %0.5f, dis: %0.5f, time: %0.3f' % (e, i, encLoss.detach().cpu().numpy(), recLoss.detach().cpu().numpy(), disLoss.detach().cpu().numpy(), time() - T))\n",
    "\n",
    "            # storing losses for plotting later\n",
    "            # losses['enc'].append(epochEncLoss/i)\n",
    "            losses['rec'].append(epochRecLoss/i)\n",
    "            # losses['dis'].append(epochDisLoss/i)\n",
    "\n",
    "            #### Test\n",
    "            dae.eval()\n",
    "            # dis.eval()\n",
    "\n",
    "            # #get test outuputs and losses\n",
    "            # xTest, yTest = prep_data(iter(testLoader).next(), useCUDA=dae.useCUDA)\n",
    "            # zTest, recTest = dae.forward(xTest)  #N.B. corruption in here\n",
    "            # recLossTest = dae.rec_loss(recTest, xTest)\n",
    "            # #Plot losses\n",
    "            # # losses['test rec'].append(recLossTest.data[0])\n",
    "            # losses['test rec'].append(recLossTest.detach().cpu().numpy())\n",
    "\n",
    "#             if e > 0: #only one point for test rec otherwise\n",
    "#                 plot_losses(losses, exDir, epochs=e+1)\n",
    "#                 plot_norm_losses(losses, exDir)\n",
    "\n",
    "            #save parameters\n",
    "            dae.save_params(exDir)\n",
    "            # dis.save_params(exDir)\n",
    "\n",
    "            #Save images of original and rec\n",
    "            # save_image(xTest.data, join(exDir, 'original.png'))\n",
    "            # save_image(recTest.data, join(exDir, 'rec.png'))\n",
    "\n",
    "            #Save samples\n",
    "            # sampleDir = join(exDir,'epoch_'+str(e))\n",
    "            # os.mkdir(sampleDir)\n",
    "            # print('sample dir:', sampleDir)\n",
    "            # dae.sample_x(args.M, sampleDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "27de6c49-c3df-4507-81fe-52382824b5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "args = Args()\n",
    "\n",
    "random.seed(args.random_seed)\n",
    "np.random.seed(args.random_seed)\n",
    "torch.manual_seed(args.random_seed)\n",
    "\n",
    "# X, labels = prepare_data(args)\n",
    "# trainLoader, testLoader = prepare_dataloaders(args, X, labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = prepare_data_PAMAP2(args)\n",
    "\n",
    "# tail = len(X_test) % args.batchSize\n",
    "# X = torch.from_numpy(X_test[:len(X_test)-tail,:,:])\n",
    "# labels = torch.from_numpy(y_test[:len(X_test)-tail,:])\n",
    "trainDataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "testDataset = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "trainLoader = torch.utils.data.DataLoader(trainDataset,\n",
    "    batch_size=args.batchSize, shuffle=True, drop_last=True) \n",
    "\n",
    "testLoader = torch.utils.data.DataLoader(testDataset,\n",
    "    batch_size=args.batchSize, shuffle=False, drop_last=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#train_activity_recognition(args, exDir, trainLoader, testLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "89150e80-1718-4a62-8cd9-e0bcabcf6c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experimental details:\n",
      "root = ../data\n",
      "batchSize = 64\n",
      "maxEpochs = 100\n",
      "nz = 200\n",
      "lr = 0.0001\n",
      "fSize = 64\n",
      "outDir = ../PAMAP2_DATASET/Optional/\n",
      "commit = eval\n",
      "alpha = 1.0\n",
      "M = 5\n",
      "loss = MSE\n",
      "loadDAE = False\n",
      "loadSVM = False\n",
      "load_DAE_from = None\n",
      "evalMode = False\n",
      "comment = \n",
      "momentum = 0.1\n",
      "c = 0.01\n",
      "svmLR = 0.0001\n",
      "Ntest = 100\n",
      "gpuNo = 0\n",
      "multimodalZ = False\n",
      "window_len = 512\n",
      "stride_len = 20\n",
      "act_list = [1, 2, 3, 4, 5, 6, 7, 12, 13, 16, 17, 24]\n",
      "imSize = 64\n",
      "sigma = [40, 80]\n",
      "cuda_id = 0\n",
      "random_seed = 2\n",
      "train_split = 0.8\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_dae(args, exDir, trainLoader, testLoader)\n",
      "Cell \u001b[1;32mIn[47], line 41\u001b[0m, in \u001b[0;36mtrain_dae\u001b[1;34m(args, exDir, trainLoader, testLoader)\u001b[0m\n\u001b[0;32m     37\u001b[0m x, y \u001b[39m=\u001b[39m prep_data(data, useCUDA\u001b[39m=\u001b[39mdae\u001b[39m.\u001b[39museCUDA)\n\u001b[0;32m     38\u001b[0m \u001b[39m# print(x.shape)\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \n\u001b[0;32m     40\u001b[0m \u001b[39m# get outputs\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m zFake, xRec \u001b[39m=\u001b[39m dae\u001b[39m.\u001b[39;49mforward(x)\n\u001b[0;32m     43\u001b[0m \u001b[39m# clac losses\u001b[39;00m\n\u001b[0;32m     44\u001b[0m recLoss \u001b[39m=\u001b[39m dae\u001b[39m.\u001b[39mrec_loss(xRec, x, loss\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mloss)  \u001b[39m#loss='BCE' or 'MSE'\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Arka Rutvik\\OneDrive\\Desktop\\New folder (2)\\DAE\\models\\de_PAMAP2.py:195\u001b[0m, in \u001b[0;36mDAE.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m    193\u001b[0m     \u001b[39m# the outputs needed for training\u001b[39;00m\n\u001b[0;32m    194\u001b[0m     \u001b[39m# x_corr = self.corrupt(x, rows =4, noise_perc=noise_perc)\u001b[39;00m\n\u001b[1;32m--> 195\u001b[0m     x_corr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcorrupt(x)\n\u001b[0;32m    196\u001b[0m     z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode(x_corr)\n\u001b[0;32m    197\u001b[0m     \u001b[39mreturn\u001b[39;00m z, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode(z)\n",
      "File \u001b[1;32mc:\\Users\\Arka Rutvik\\OneDrive\\Desktop\\New folder (2)\\DAE\\models\\de_PAMAP2.py:88\u001b[0m, in \u001b[0;36mDAE.corrupt\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcorrupt\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 88\u001b[0m     noise \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msigma \u001b[39m*\u001b[39;49m torch\u001b[39m.\u001b[39;49mrandn(x\u001b[39m.\u001b[39;49msize())\u001b[39m.\u001b[39;49mtype_as(x)\n\u001b[0;32m     89\u001b[0m     \u001b[39mreturn\u001b[39;00m x \u001b[39m+\u001b[39m noise\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "train_dae(args, exDir, trainLoader, testLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc856f48-e36b-4cb7-b4dc-52f8f8c9d25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs will be saved to: ../PAMAP2_DATASET/Optional/Ex_2\n"
     ]
    }
   ],
   "source": [
    "exDir = make_new_folder(args.outDir)\n",
    "print('Outputs will be saved to:', exDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ea9137-179b-4265-96bc-d93ed139040e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d4bdb2-3703-419d-9138-1c76d304a850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97bdfef-a87f-46e3-b1db-9c3e7bb4b0dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf09f260-3135-4417-9f9a-efa1154ff8c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
