{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c25dc8-18db-4931-8d8f-8a833af4c6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as cp\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import metrics,mean_squared_error,confusion_matrix\n",
    "from utils.function import prep_data\n",
    "from utils.sliding_window import sliding_window\n",
    "from models.de_HHAR import DAE\n",
    "from models.activity_recognition import *\n",
    "from models.dis_z import DIS_Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d944f1-784f-4210-9546-deaf048b6fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.root = '../data'\n",
    "        self.batchSize = 64\n",
    "        self.maxEpochs = 100\n",
    "        self.nz = 200\n",
    "        self.lr = 1e-4\n",
    "        self.fSize = 64\n",
    "        self.outDir = 'data/experiments/DAE_EVAL_HHAR_Noise'\n",
    "        self.commit = 'eval'\n",
    "        self.alpha = 1.0\n",
    "        self.M = 5\n",
    "        self.loss = 'MSE' #'BCE'\n",
    "        self.loadDAE = False\n",
    "        self.loadSVM = False    \n",
    "        self.load_DAE_from = None\n",
    "        self.evalMode = False\n",
    "        self.comment = ''\n",
    "        self.momentum = 0.1\n",
    "        self.c = 0.01\n",
    "        self.svmLR = 1e-4\n",
    "        self.Ntest = 100\n",
    "        self.gpuNo = 0\n",
    "        self.multimodalZ = False\n",
    "        self.imSize = 64\n",
    "        self.sigma=[0.2]\n",
    "        # self.sigma = [0.3, 30, 50]\n",
    "        self.cuda_id = 0\n",
    "        self.train_split = 0.8\n",
    "        self.dae_model_loc = \"data/experiments/DAE_EVAL_HHAR_Noise/Ex_10\" \n",
    "        self.ar_model_loc = \"../../../../HHAR_Time_ConvAttn.pt\"\n",
    "random_seed = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf4fc7a-974d-424c-8c05-567b447e020b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_id = 1\n",
    "\n",
    "if gpu_id>=0:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "    cuda_id = \"cuda:\" + str(0)  # cuda:2\n",
    "\n",
    "device = torch.device(cuda_id if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "if (torch.cuda.is_available()):\n",
    "    torch.cuda.set_device(cuda_id)\n",
    "    print(\"Current GPU ID:\", torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff755101-ccb9-44fb-9b83-7011f4c7ce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dis(args, dae, multimodalZ):\n",
    "    if not multimodalZ:\n",
    "        print('\\n ** USING NORMAL PRIOR **')\n",
    "        prior = dae.norm_prior\n",
    "        NZ = args.nz\n",
    "    else:\n",
    "        print('\\n ** USING MULTIMODAL PRIOR **')\n",
    "        prior = dae.multi_prior\n",
    "        NZ = 2\n",
    "    dis = DIS_Z(nz=NZ, prior=prior)\n",
    "\n",
    "    return dis, NZ\n",
    "\n",
    "\n",
    "def analysis(args, dae, testDataset, X, num_classes):\n",
    "    # Prepare testdata set, drop the last incomplete batch\n",
    "    test_x = torch.zeros(len(testDataset), X.shape[1], X.shape[2], X.shape[3])\n",
    "    test_labels = torch.zeros(len(testDataset), num_classes)\n",
    "    for test_id in range(len(testDataset)):\n",
    "        test_labels[test_id] = testDataset[test_id][1]\n",
    "        test_x[test_id] = testDataset[test_id][0]\n",
    "    print(test_x.shape)\n",
    "\n",
    "    # Corrupt dataset\n",
    "    corr_test_x = dae.corrupt(test_x)\n",
    "    \n",
    "    mean_fill_test = interpolation_meanfilling(corr_test_x)\n",
    "    linear_interp_test = linear_interpolation(corr_test_x)\n",
    "# recon_test -> synthesize the entire dataset\n",
    "    z = dae.encode(corr_test_x)\n",
    "    recon_test = dae.decode(z)\n",
    "\n",
    "    # breakpoint()\n",
    "# reconstruct filling testset (fill missing values only, not valid for noisy data)\n",
    "    recon_fill_test = copy.deepcopy(corr_test_x).detach().cpu().numpy()\n",
    "    np.copyto(recon_fill_test, recon_test.detach().numpy(), where = recon_fill_test==0)\n",
    "    recon_fill_test = torch.from_numpy(recon_fill_test)\n",
    "\n",
    "    # Reconstruct testset\n",
    "    # corr_test_dataset = TensorDataset(outputs.permute(0, 2, 1))\n",
    "    #recon_test = torch.from_numpy(recon_test)#.permute(1, 2, 0)\n",
    "    \n",
    "    # test_x = test_x.reshape(test_x.shape[0],test_x.shape[2],test_x.shape[3])\n",
    "    # corr_test_x = corr_test_x.reshape(corr_test_x.shape[0],corr_test_x.shape[2],corr_test_x.shape[3])\n",
    "    # recon_test = recon_test.reshape(recon_test.shape[0],recon_test.shape[2],recon_test.shape[3])\n",
    "    # recon_fill_test = recon_fill_test.reshape(recon_fill_test.shape[0],recon_fill_test.shape[2],recon_fill_test.shape[3])\n",
    "    # mean_fill_test = mean_fill_test.reshape(mean_fill_test.shape[0],mean_fill_test.shape[2],mean_fill_test.shape[3])\n",
    "    # test_x = test_x.permute(0,1,3,2)\n",
    "    # corr_test_x = corr_test_x.permute(0,1,3,2)\n",
    "    # recon_test = recon_test.permute(0,1,3,2)\n",
    "    # recon_fill_test = recon_fill_test.permute(0,1,3,2)\n",
    "    # mean_fill_test = mean_fill_test.permute(0,1,3,2)\n",
    "\n",
    "\n",
    "    raw_test_dataset = TensorDataset(test_x, test_labels)\n",
    "    corr_test_dataset = TensorDataset(corr_test_x, test_labels)\n",
    "    recon_test_dataset = TensorDataset(recon_test, test_labels)\n",
    "    recon_fill_test_dataset = TensorDataset(recon_fill_test, test_labels)\n",
    "    mean_fill_test_dataset = TensorDataset(mean_fill_test, test_labels)\n",
    "    linear_interp_test_dataset = TensorDataset(linear_interp_test, test_labels)\n",
    "\n",
    "    return test_x, corr_test_x, recon_test, recon_fill_test, mean_fill_test, linear_interp_test, raw_test_dataset, corr_test_dataset, recon_test_dataset, recon_fill_test_dataset, mean_fill_test_dataset,linear_interp_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dde417-c06c-44aa-90a2-c258fe96c50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Sensor Channels used in the OPPORTUNITY dataset.\n",
    "NB_SENSOR_CHANNELS = 6\n",
    "\n",
    "# Number of classes in which data is classified (or to be classified).\n",
    "NUM_CLASSES = 6\n",
    "\n",
    "# Length of the sliding window used to segmenting the time-series-data.\n",
    "SLIDING_WINDOW_LENGTH = 100\n",
    "\n",
    "act_labels_txt = ['std', 'wlk', 'sit', 'lie', 'null']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927e31c1-658d-47e3-aa9e-6afb520b4932",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_path='../../../../Dataset/'\n",
    "X_total = np.load(data_folder_path + 'hhar_time_X.npy').astype('float32')\n",
    "y_total = np.load(data_folder_path + 'hhar_time_y.npy').astype(int)\n",
    "\n",
    "X_total=np.nan_to_num(X_total)\n",
    "for i in range(X_total.shape[1]):\n",
    "    ch_data = X_total[:,i,:] # the data of channel id\n",
    "    scaler = MinMaxScaler()\n",
    "    ch_data = scaler.fit_transform(ch_data) # scale the data in this channel to [0,1]\n",
    "    X_total[:,i,:] = ch_data # assign normalized data to normalized_X \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_total, y_total, test_size=0.2, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f37a1a-445c-4505-854b-35039aa6edc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1], X_train.shape[2])\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1], X_test.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac638067-7240-4159-b37b-c2532899ca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolation_meanfilling(corr_test_x):\n",
    "    mean_fill_test = copy.deepcopy(corr_test_x).detach().cpu().numpy()\n",
    "    for i in range(mean_fill_test.shape[0]):\n",
    "        for j in range(mean_fill_test.shape[2]):\n",
    "            if np.count_nonzero(mean_fill_test[i,:,j,:]) == 0:  \n",
    "                ch_mean = 0\n",
    "            else:\n",
    "             #ch_mean = np.sum(mean_fill_test[i][0][j]) / np.count_nonzero(mean_fill_test[i][0][j])\n",
    "                ch_mean = np.sum(mean_fill_test[i,:,j,:]) / np.count_nonzero(mean_fill_test[i,:,j,:])\n",
    "                mean_fill_test[i,:,j,:][mean_fill_test[i,:,j,:] == 0] = ch_mean\n",
    "    mean_fill_test = torch.from_numpy(mean_fill_test)\n",
    "    return mean_fill_test\n",
    "\n",
    "def linear_interpolation(corr_test_x):\n",
    "    linear_interp_test = copy.deepcopy(corr_test_x).detach().cpu().numpy()\n",
    "\n",
    "    linear_interp_test = linear_interp_test.reshape(-1, linear_interp_test.shape[3],linear_interp_test.shape[2])\n",
    "    for i in range(linear_interp_test.shape[0]):\n",
    "        for j in range(linear_interp_test.shape[2]):\n",
    "            if np.count_nonzero(linear_interp_test[i,:,j]) == 0: # when all data points in this channel are missing\n",
    "                linear_interp_test[i, :, j] = 0.0\n",
    "            else:\n",
    "                idxs = np.arange(linear_interp_test.shape[1]) # indexes of all the samples\n",
    "                zero_filter = linear_interp_test[i,:,j] == 0 # index filter for zero values\n",
    "                zero_idxs = idxs[zero_filter] # indexes for zero values\n",
    "                non_zero_idxs = idxs[~zero_filter] # xp, indexes for non-zero values\n",
    "                non_zero_vals = linear_interp_test[i, ~zero_filter,j] # fp, non-zero values\n",
    "                interp_vals = np.interp(zero_idxs, non_zero_idxs, non_zero_vals) # interpolated values\n",
    "                linear_interp_test[i,zero_idxs,j] = interp_vals # fill interpolated values to the corrupted signal\n",
    "    linear_interp_test = torch.from_numpy(linear_interp_test)\n",
    "    linear_interp_test = torch.reshape(linear_interp_test, (-1, 1, linear_interp_test.shape[2], linear_interp_test.shape[1]))\n",
    "    return linear_interp_test\n",
    "\n",
    "\n",
    "def evaluate_rmse(corr_test_x, recon_test, recon_fill_test,mean_fill_test,linear_interp_test,test_x):\n",
    "    corr_rms = mean_squared_error(test_x.reshape(test_x.shape[0],-1).cpu().detach().numpy(), corr_test_x.reshape(corr_test_x.shape[0],-1).cpu().detach().numpy(), squared=False)\n",
    "    print('Corr RMSE:\\n' + str(corr_rms))\n",
    "\n",
    "    recon_rms = mean_squared_error(test_x.reshape(test_x.shape[0],-1).cpu().detach().numpy(), recon_test.reshape(recon_test.shape[0],-1).cpu().detach().numpy(), squared=False)\n",
    "    print('Recon RMSE:\\n' + str(recon_rms))\n",
    "\n",
    "    recon_fill_rms = mean_squared_error(test_x.reshape(test_x.shape[0],-1).cpu().detach().numpy(), recon_fill_test.reshape(recon_fill_test.shape[0],-1).cpu().detach().numpy(), squared=False)\n",
    "    print('Recon fill RMSE:\\n' + str(recon_fill_rms))\n",
    "\n",
    "    mean_fill_rms = mean_squared_error(test_x.reshape(test_x.shape[0],-1).cpu().detach().numpy(), mean_fill_test.reshape(mean_fill_test.shape[0],-1).cpu().detach().numpy(), squared=False)\n",
    "    print('Mean Fill RMSE:\\n' + str(mean_fill_rms))\n",
    "    \n",
    "    linear_interp_rms = mean_squared_error(test_x.reshape(test_x.shape[0],-1), linear_interp_test.reshape(linear_interp_test.shape[0],-1).cpu().detach().numpy(), squared=False)\n",
    "    print('Linear Interpolation RMSE:\\n' + str(linear_interp_rms))\n",
    "    return \n",
    "#def plot(test_x, corr_test_x, recon_test)\n",
    "\n",
    "def plot(test_x, corr_test_x, recon_test, recon_fill_test,mean_fill_test, linear_interp_test):\n",
    "    plt.imshow(test_x[0][0].detach())\n",
    "    plt.title('Raw Data')\n",
    "   # plt.subplot(4,1,1)\n",
    "    plt.savefig(join(args.dae_model_loc, 'raw.png'))\n",
    "\n",
    "    plt.imshow(corr_test_x[0][0].detach())\n",
    "    plt.title('Corrupted')\n",
    "   # plt.subplot(4,1,2)\n",
    "    plt.savefig(join(args.dae_model_loc, 'corr.png'))\n",
    "\n",
    "    plt.imshow(recon_test[0][0].detach())\n",
    "    plt.title('Reconstructed')\n",
    "   # plt.subplot(4,1,3)\n",
    "    plt.savefig(join(args.dae_model_loc, 'reconstructed.png'))\n",
    "\n",
    "    plt.imshow(recon_fill_test[0][0].detach())\n",
    "    plt.title('Reconstructed fill')\n",
    "   # plt.subplot(4,1,3)\n",
    "    plt.savefig(join(args.dae_model_loc, 'rec_fill.png'))\n",
    "\n",
    "    plt.imshow(mean_fill_test[0][0].detach())\n",
    "    plt.title('Mean Fill')\n",
    "   # plt.subplot(4,1,4)\n",
    "    plt.savefig(join(args.dae_model_loc, 'mean.png'))\n",
    "    \n",
    "    plt.imshow(linear_interp_test[0][0].detach())\n",
    "    plt.title('Linear Interp')\n",
    "   # plt.subplot(4,1,4)\n",
    "    plt.savefig(join(args.dae_model_loc, 'linear.png'))\n",
    "    \n",
    "    return\n",
    "\n",
    "def test_dae(args, path, trainLoader, testLoader):\n",
    "    pass\n",
    "\n",
    "\n",
    "def test_activity_recognition(args, path, trainLoader, testLoader):\n",
    "    pass\n",
    "\n",
    "\n",
    "def calculate_combined_accuracy(args, test_loader, sigma):\n",
    "    # device = torch.device(args.cuda_id if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    args.n_sensor_channels = 6\n",
    "    args.len_seq = 100\n",
    "    args.num_classes=6\n",
    "    ar = get_eval_model(n_sensor_channels=args.n_sensor_channels, len_seq=args.len_seq, num_classes=args.num_classes, model_path=args.ar_model_loc) #n_sensor_channels, len_seq, num_classes, model_path\n",
    "    ar = ar.to(device)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_true = []\n",
    "    total_pred = []\n",
    "\n",
    "    with torch.no_grad():   \n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)              \n",
    "            outputs = ar(images)\n",
    "            predicted = torch.argmax(outputs.data, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == torch.argmax(labels, dim=1)).sum().item()\n",
    "            \n",
    "            total_pred = total_pred + predicted.cpu().numpy().tolist()\n",
    "            total_true = total_true + (torch.argmax(labels, dim=1).cpu().numpy().tolist())\n",
    "            \n",
    "    print(f'Test Accuracy: \\n{100.0 * correct / total} ')\n",
    "    \n",
    "    # print(\" | \".join(act_labels_txt))\n",
    "    # conf_mat = confusion_matrix(y_true = total_true, y_pred = total_pred)\n",
    "    # conf_mat = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]\n",
    "    # print(np.array(conf_mat).round(3) * 100)  \n",
    "    f1_score = metrics.f1_score(y_true = total_true, y_pred = total_pred, average='weighted')\n",
    "    print('F1 score:\\n', f1_score)\n",
    "    print('')\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f846e27-c581-4188-a91f-50a5313dc137",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args = Args()\n",
    "    if args.multimodalZ:\n",
    "        args.nz = 2\n",
    "    args.random_seed = random_seed\n",
    "    args.num_classes = 6\n",
    "    random.seed(args.random_seed)\n",
    "    np.random.seed(args.random_seed)\n",
    "    torch.manual_seed(args.random_seed)\n",
    "    \n",
    "    testDataset = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "\n",
    "    testLoader = torch.utils.data.DataLoader(testDataset,\n",
    "        batch_size=args.batchSize, shuffle=False, drop_last=True)    \n",
    "    \n",
    "    dae = DAE(nz=args.nz, imSize=args.imSize, fSize=args.fSize, sigma=args.sigma, multimodalZ=args.multimodalZ)\n",
    "    dae.load_params(args.dae_model_loc)\n",
    "\n",
    "    test_x, corr_test_x, recon_test, recon_fill_test, mean_fill_test, linear_interp_test, raw_test_dataset, corr_test_dataset, recon_test_dataset, recon_fill_test_dataset, mean_fill_test_dataset, linear_interp_test_dataset = analysis(args, dae, testDataset, X_test, args.num_classes)\n",
    "\n",
    "    print(\"Raw testset:\")\n",
    "    raw_test_loader = torch.utils.data.DataLoader(raw_test_dataset,\n",
    "    batch_size= args.batchSize, shuffle=False)\n",
    "    calculate_combined_accuracy(args, raw_test_loader, sigma=args.sigma)\n",
    "\n",
    "    print(\"Corrupted testset:\")\n",
    "    corr_test_loader = torch.utils.data.DataLoader(corr_test_dataset,\n",
    "    batch_size= args.batchSize, shuffle=False)\n",
    "    calculate_combined_accuracy(args,corr_test_loader, sigma=args.sigma)\n",
    "\n",
    "    print(\"Reconstructed testset:\")\n",
    "    recon_test_loader = torch.utils.data.DataLoader(recon_test_dataset,\n",
    "    batch_size=args.batchSize, shuffle=False)\n",
    "    calculate_combined_accuracy(args,recon_test_loader, sigma=args.sigma)\n",
    "\n",
    "    print(\"Reconstructed fill testset:\")\n",
    "    recon_fill_test_loader = torch.utils.data.DataLoader(recon_fill_test_dataset,\n",
    "    batch_size=args.batchSize, shuffle=False)\n",
    "    calculate_combined_accuracy(args,recon_fill_test_loader, sigma=args.sigma)\n",
    "\n",
    "    print(\"Mean Fill testset\")\n",
    "    mean_fill_test_loader = torch.utils.data.DataLoader(mean_fill_test_dataset,\n",
    "    batch_size=args.batchSize, shuffle=False)\n",
    "    calculate_combined_accuracy(args,mean_fill_test_loader, sigma=args.sigma)\n",
    "    \n",
    "    print(\"Linear Interpolation testset:\")\n",
    "    linear_interp_test_loader = torch.utils.data.DataLoader(linear_interp_test_dataset,\n",
    "    batch_size=args.batchSize, shuffle=False)\n",
    "    calculate_combined_accuracy(args,linear_interp_test_loader, sigma=args.sigma)\n",
    "\n",
    "    evaluate_rmse(corr_test_x, recon_test,recon_fill_test, mean_fill_test, linear_interp_test,test_x)\n",
    "    plot(test_x, corr_test_x, recon_test,recon_fill_test, mean_fill_test, linear_interp_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
